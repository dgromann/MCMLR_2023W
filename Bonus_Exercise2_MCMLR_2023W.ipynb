{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGkhOOUusAVIZIszAnnuQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/MCMLR_2023W/blob/main/Bonus_Exercise2_MCMLR_2023W.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Exercises 2: Explore Language Model Vocabulary**\n",
        "\n",
        "This notebook represents the second bonus exercise for the lecture Multilingual and Crosslingual Methods and Language Resources (2023W 340168-1). For each of the bonus exercises you can obtain a maximum of 3 points that are added to the points of your final exam. The sections where you need to complete the code are marked with üëã ‚öí.\n"
      ],
      "metadata": {
        "id": "Uem6oQr40aBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explore LMs internal vocabularies**\n",
        "\n",
        "This exercsise loads pretrained language models and allows you to explore their vocabulary. As always, we first need to load the transformers library to access the transformer models on HuggingFace.\n"
      ],
      "metadata": {
        "id": "54vas_gz00f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "ecWk1yvvNnjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will need to load the first model, which is the multilingual XLM-RoBERTa-base model for this tutorial."
      ],
      "metadata": {
        "id": "ET89loQJObtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\" )"
      ],
      "metadata": {
        "id": "02oQcUarN4NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will explore the vocabulary of the model by using the function `tokenizer.get_vocab()`. Keep in mind that the tokenizer stores tokens and identifiers. Here we are mainly interested in exploring the tokens.\n",
        "\n",
        "üëã ‚öí Explore the following aspects about the vocabulary of XLM-R:\n",
        "\n",
        "\n",
        "*   What datatype does the function `tokenizer.get_vocab()`return?\n",
        "*   How many tokens are in the vocabulary of XLM-R?\n",
        "*   When representing all tokens as a list, which token is stored on position 300?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K7kCv1DkOqw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code should go here\n",
        "xlmr_tokenizer.get_vocab()"
      ],
      "metadata": {
        "id": "g33UsCK6Oy9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a next step we want to store the entire vocabulary.\n",
        "\n",
        "üëã ‚öí Run through the vocabulary and write each token on a separate line in a local file, i.e., store all line-separated tokens in a file.  "
      ],
      "metadata": {
        "id": "M7Ks8NprRRrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code to write tokens separated by line to a file should go here"
      ],
      "metadata": {
        "id": "XyNHsry4RnBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SentencePiece vs. WordPiece Tokenization**\n",
        "\n",
        "We have explored the WordPiece before when investigating the vocabulary of BERT. In contrast to BERT, XLM-R uses SentencePiece tokenization. Here we will explore the difference between these two. Both try to separate the intput into subword tokens.\n",
        "\n",
        "When looking at the tokenization of the word \"philosphy\", the output of the two tokenizers is:  \n",
        "\n",
        "```\n",
        "WordPiece:  phil  ##os  ##phy\n",
        "SentencePiece:\t‚ñÅphil   os    phy\n",
        "```\n",
        "\n",
        "In WordPiece, subwords are denoted by two hash characters, except the *first* subword in a word.\n",
        "\n",
        "Instead of splitting words based on spaces, SentencePiece treats the input as a raw input stream and includes spaces in its representation in the form of _.  Decoding with SentencePiece is very easy since all tokens can just be concatenated and \"‚ñÅ\" is replaced by a space.\n",
        "\n",
        "For a more detailed description of the two tokenizers, please see the [HuggingFace Summary of Tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece).\n",
        "\n",
        "To explore the difference with some examples, we need to load the BERT-base model as well.\n"
      ],
      "metadata": {
        "id": "1l5hUmQ3RsLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "TpPy7x2tTYtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Compare the two tokenizers `bert_tokenizer`and `xlmr_tokenizer`by tokenizing the following `example_sentence` and output the result."
      ],
      "metadata": {
        "id": "eLy8tIFJTkPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"I don't even need a GPU for this!\"\n",
        "\n",
        "#Your code should go here\n"
      ],
      "metadata": {
        "id": "5SVI-E-VT2WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí What difference can you observe between the way these two tokenizers tokenize the example sentence?"
      ],
      "metadata": {
        "id": "criWhezsUa-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will try to improve the output format of the vocabulary comparison.\n",
        "\n",
        "üëã ‚öí Write a function that allows us to directly compare the two outputs by assigning IDs to each element in the list and printing the tokens of both per index position, i.e., ID. You can do this by creating a print format yourself or using other libraries, e.g. Pandas Dataframes.  "
      ],
      "metadata": {
        "id": "Bp0SCpIQUhiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_tokenizer_outputs(xlmr_tokens, bert_tokens):\n"
      ],
      "metadata": {
        "id": "2ouFYfXXVOeF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Single Character Tokens**\n",
        "\n",
        "All single character tokens in the vocabulary are not preceded by an underscore.\n",
        "\n",
        "üëã ‚öí Write a function that finds all the single character tokens in the XLM-R vocabulary and then prints them in a way that 30 characters are printed in per line.\n"
      ],
      "metadata": {
        "id": "44JDiYNeWCbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "_PQioAQEWrBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explore Token Length**\n",
        "\n",
        "In this section, we will explore the distribution of token length.\n",
        "\n",
        "üëã ‚öí Create a list that contains the length of each token called `token_lengths`, which is then passed to the Pandas Dataframe to visualize the length distribution with a seaborn count plot."
      ],
      "metadata": {
        "id": "pW9SD2sLWtVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Set the plot and font size\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# Create your list of token_lengths here\n",
        "\n",
        "df = pd.DataFrame(token_lengths, columns=[\"Token Length\"])\n",
        "\n",
        "# Plot the number of tokens of each length\n",
        "sns.countplot(df, x= \"Token Length\")\n",
        "plt.title('Vocab Token Lengths')\n",
        "plt.xlabel('Token Length')\n",
        "plt.ylabel('# of Tokens')\n",
        "\n",
        "print('Maximum token length:', max(token_lengths), '\\n\\n')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MNc6aTPpW2Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Write a few lines of code or a function that will output all tokens of the XLM-R model of length 16 exactly."
      ],
      "metadata": {
        "id": "LRdqy-UO15f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "h65dQs5P2Asq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **English Words**\n",
        "In this section, we will use WordNet to identify all the tokens that represent English words."
      ],
      "metadata": {
        "id": "MvOV57D62Ooj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7UMdeaa2ZTP",
        "outputId": "afbb61e5-d64b-4e47-d9e0-f93975b6934d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Write a function that runs through all the tokens and checks if they are in WordNet or not. If they are in WordNet, append them to a list that you return as a result of your function. Then output the length of the list of English words, the percentage they make up compared to the total number of tokens, and print the first ten words in the list."
      ],
      "metadata": {
        "id": "UEp5uzfO2i9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "iXGjOVC_2iXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}